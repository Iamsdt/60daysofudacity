{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LanguageModeling using torchtext.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hk8_aemE8cXs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torchtext import data as d\n",
        "from torchtext import datasets\n",
        "from torchtext.vocab import GloVe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46_W3DBL8nwd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6f751a04-742a-4118-db8d-9007a78749f8"
      },
      "source": [
        "is_cuda = torch.cuda.is_available()\n",
        "is_cuda"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4j_nTcy8puW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT = d.Field(lower=True, batch_first=True,)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLNIlBuC8rt3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "2d757fa9-10ba-49f9-9273-765bf5fa9840"
      },
      "source": [
        "# make splits for data\n",
        "train, valid, test = datasets.WikiText2.splits(TEXT,root='data')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading wikitext-2-v1.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "wikitext-2-v1.zip: 100%|██████████| 4.48M/4.48M [00:00<00:00, 6.71MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "extracting\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXTtEd1_81m8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size=20\n",
        "bptt_len=30\n",
        "clip = 0.25\n",
        "lr = 20\n",
        "log_interval = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDzhg_1L84Hz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e86122dc-1b84-464d-879c-62f2df801004"
      },
      "source": [
        "(len(valid[0].text)//batch_size)*batch_size"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "217640"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwwSNuL886Tj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "17eec7a2-aeb4-4dcb-acc8-9c6ddfe5e05a"
      },
      "source": [
        "len(valid[0].text)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "217646"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq5-7vPP89R9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train[0].text = train[0].text[:(len(train[0].text)//batch_size)*batch_size]\n",
        "valid[0].text = valid[0].text[:(len(valid[0].text)//batch_size)*batch_size]\n",
        "test[0].text = test[0].text[:(len(valid[0].text)//batch_size)*batch_size]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kZXA1QZ8_lt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ce2c1324-e0b0-4608-b0f0-a4e4ac920ed0"
      },
      "source": [
        "len(valid[0].text)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "217640"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mc4gO6d9B9B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "7939e98e-c3ef-4de5-f1b4-33880380e749"
      },
      "source": [
        "# print information about the data\n",
        "print('train.fields', train.fields)\n",
        "print('len(train)', len(train))\n",
        "print('vars(train[0])', vars(train[0])['text'][0:10])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train.fields {'text': <torchtext.data.field.Field object at 0x7f43e511d550>}\n",
            "len(train) 1\n",
            "vars(train[0]) ['<eos>', '=', 'valkyria', 'chronicles', 'iii', '=', '<eos>', '<eos>', 'senjō', 'no']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyfxdpFz9C3M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT.build_vocab(train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWqDicG-9Erg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4ac06769-685a-4693-89ab-07019586ddcc"
      },
      "source": [
        "print('len(TEXT.vocab)', len(TEXT.vocab))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len(TEXT.vocab) 28913\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AaL5Diy9H0T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "74934d62-14d7-4acf-9c77-9e8d67f5c173"
      },
      "source": [
        "train_iter, valid_iter, test_iter = d.BPTTIterator.splits((train, valid, test), batch_size=batch_size, bptt_len=bptt_len, device=0,repeat=False)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
            "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
            "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLlyRgAJ9Iko",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNModel(nn.Module):\n",
        "    def __init__(self,ntoken,ninp,nhid,nlayers,dropout=0.5,tie_weights=False):\n",
        "        super().__init__()\n",
        "        self.drop = nn.Dropout()\n",
        "        self.encoder = nn.Embedding(ntoken,ninp)\n",
        "        self.rnn = nn.LSTM(ninp,nhid,nlayers,dropout=dropout)\n",
        "        self.decoder = nn.Linear(nhid,ntoken)\n",
        "        if tie_weights:\n",
        "            self.decoder.weight = self.encoder.weight\n",
        "        \n",
        "        self.init_weights()\n",
        "        self.nhid = nhid\n",
        "        self.nlayers = nlayers\n",
        "        \n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange,initrange)\n",
        "        self.decoder.bias.data.fill_(0)\n",
        "        self.decoder.weight.data.uniform_(-initrange,initrange)\n",
        "        \n",
        "    def forward(self,input,hidden): \n",
        "        \n",
        "        emb = self.drop(self.encoder(input))\n",
        "        output,hidden = self.rnn(emb,hidden)\n",
        "        output = self.drop(output)\n",
        "        s = output.size()\n",
        "        decoded = self.decoder(output.view(s[0]*s[1],s[2]))\n",
        "        return decoded.view(s[0],s[1],decoded.size(1)),hidden\n",
        "    \n",
        "    def init_hidden(self,bsz):\n",
        "        weight = next(self.parameters()).data\n",
        "        return(Variable(weight.new(self.nlayers,bsz,self.nhid).zero_()),Variable(weight.new(self.nlayers,bsz,self.nhid).zero_()))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0qbGp-H9LAJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdJ_6YHG9Nry",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2a0d8db2-3003-4435-9a1c-042594262692"
      },
      "source": [
        "len(valid_iter.dataset[0].text)\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "217640"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgwUx0fZ9U4I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emsize = 200\n",
        "nhid=200\n",
        "nlayers=2\n",
        "dropout = 0.2\n",
        "\n",
        "ntokens = len(TEXT.vocab)\n",
        "lstm = RNNModel(ntokens, emsize, nhid,nlayers, dropout, 'store_true')\n",
        "if is_cuda:\n",
        "    lstm = lstm.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkDlgy3g9Wm5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
        "    if type(h) == Variable:\n",
        "        return Variable(h.data)\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqpBjqbk9ZGv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def evaluate(data_source):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    lstm.eval()\n",
        "    total_loss = 0   \n",
        "    hidden = lstm.init_hidden(batch_size)\n",
        "    for batch in data_source:        \n",
        "        data, targets = batch.text,batch.target.view(-1)\n",
        "        output, hidden = lstm(data, hidden)\n",
        "        output_flat = output.view(-1, ntokens)\n",
        "        total_loss += len(data) * criterion(output_flat, targets).data\n",
        "        hidden = repackage_hidden(hidden)\n",
        "    return total_loss[0]/(len(data_source.dataset[0].text)//batch_size) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZWHPgOS9aCI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainf():\n",
        "    # Turn on training mode which enables dropout.\n",
        "    lstm.train()\n",
        "    total_loss = 0\n",
        "    start_time = time.time()\n",
        "    hidden = lstm.init_hidden(batch_size)\n",
        "    for  i,batch in enumerate(train_iter):\n",
        "        data, targets = batch.text,batch.target.view(-1)\n",
        "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "        hidden = repackage_hidden(hidden)\n",
        "        lstm.zero_grad()\n",
        "        output, hidden = lstm(data, hidden)\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_norm(lstm.parameters(), clip)\n",
        "        for p in lstm.parameters():\n",
        "            p.data.add_(-lr, p.grad.data)\n",
        "\n",
        "        total_loss += loss.data\n",
        "\n",
        "        if i % log_interval == 0 and i > 0:\n",
        "            cur_loss = total_loss[0] / log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            (print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | loss {:5.2f} | ppl {:8.2f}'.format(epoch, i, len(train_iter), lr,elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss))))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umHFP4hs9b9y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loop over epochs.\n",
        "best_val_loss = None\n",
        "epochs = 40\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "    epoch_start_time = time.time()\n",
        "    trainf()\n",
        "    val_loss = evaluate(valid_iter)\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "        'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                   val_loss, math.exp(val_loss)))\n",
        "    print('-' * 89)\n",
        "    if not best_val_loss or val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "    else:\n",
        "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "        lr /= 4.0"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}