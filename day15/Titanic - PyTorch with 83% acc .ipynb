{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "f66eed17ca77cd4973b8f00c510c5d98e4dc2af7"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('../input/train.csv')\n",
    "testset = pd.read_csv('../input/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chnage Name into 3 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "4db62430cd0965249cd85567a0a22a59e56b02ad"
   },
   "outputs": [],
   "source": [
    "dataset_title = [i.split(',')[1].split('.')[0].strip() for i in dataset['Name']]\n",
    "dataset['Title'] = pd.Series(dataset_title)\n",
    "dataset['Title'].value_counts()\n",
    "dataset['Title'] = dataset['Title'].replace(['Lady', 'the Countess', 'Countess', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona', 'Ms', 'Mme', 'Mlle'], 'Others')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset_title = [i.split(',')[1].split('.')[0].strip() for i in testset['Name']]\n",
    "testset['Title'] = pd.Series(dataset_title)\n",
    "testset['Title'].value_counts()\n",
    "testset['Title'] = testset['Title'].replace(['Lady', 'the Countess', 'Countess', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona', 'Ms', 'Mme', 'Mlle'], 'Others')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Family Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset['FN'] = dataset['SibSp'] + dataset['Parch'] + 1\n",
    "testset['FN'] = testset['SibSp'] + testset['Parch'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "06bcd62c208a9f4987b5d9ae182163d1d36e2eae"
   },
   "outputs": [],
   "source": [
    "def family(x):\n",
    "    if x < 2:\n",
    "        return 'S'\n",
    "    elif x == 2:\n",
    "        return 'C'\n",
    "    elif x <= 4:\n",
    "        return 'M'\n",
    "    else:\n",
    "        return 'L'\n",
    "    \n",
    "dataset['FN'] = dataset['FN'].apply(family)\n",
    "testset['FN'] = testset['FN'].apply(family)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill NUll Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "82dbbb08eadc6bef94dbcf018ee60fdff8288668"
   },
   "outputs": [],
   "source": [
    "dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace=True)\n",
    "testset['Embarked'].fillna(testset['Embarked'].mode()[0], inplace=True)\n",
    "dataset['Age'].fillna(dataset['Age'].median(), inplace=True)\n",
    "testset['Age'].fillna(testset['Age'].median(), inplace=True)\n",
    "testset['Fare'].fillna(testset['Fare'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "620ce83ee4ff808ea787ef08583575ef15d63888"
   },
   "outputs": [],
   "source": [
    "dataset = dataset.drop(['PassengerId', 'Cabin', 'Name', 'SibSp', 'Parch', 'Ticket'], axis=1)\n",
    "testset_passengers = testset['PassengerId']\n",
    "testset = testset.drop(['PassengerId', 'Cabin', 'Name', 'SibSp', 'Parch', 'Ticket'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "ef54d377d5abe31de761a2543c373b0c510ae401"
   },
   "outputs": [],
   "source": [
    "X_train = dataset.iloc[:, 1:9].values\n",
    "Y_train = dataset.iloc[:, 0].values\n",
    "X_test = testset.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#print(Y_train.shape)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode string to number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "813ccdbc8e16e80a7401157e56523a5c75c534f0"
   },
   "outputs": [],
   "source": [
    "# Converting the remaining labels to numbers\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "X_train[:, 1] = labelencoder_X_1.fit_transform(X_train[:, 1])\n",
    "X_train[:, 4] = labelencoder_X_1.fit_transform(X_train[:, 4])\n",
    "X_train[:, 5] = labelencoder_X_1.fit_transform(X_train[:, 5])\n",
    "X_train[:, 6] = labelencoder_X_1.fit_transform(X_train[:, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "labelencoder_X_2 = LabelEncoder()\n",
    "X_test[:, 1] = labelencoder_X_2.fit_transform(X_test[:, 1])\n",
    "X_test[:, 4] = labelencoder_X_2.fit_transform(X_test[:, 4])\n",
    "X_test[:, 5] = labelencoder_X_2.fit_transform(X_test[:, 5])\n",
    "X_test[:, 6] = labelencoder_X_2.fit_transform(X_test[:, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(X_test.shape)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_test[:10, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "dc53b0cc7ab0b0130f2933eba0f1aabb05025518"
   },
   "outputs": [],
   "source": [
    "# Converting categorical values to one-hot representation\n",
    "#0, 1, 4, 5, 6\n",
    "one_hot_encoder = OneHotEncoder(categorical_features = [0, 1, 4, 5, 6])\n",
    "X_train = one_hot_encoder.fit_transform(X_train).toarray()\n",
    "X_test = one_hot_encoder.fit_transform(X_test).toarray()\n",
    "\n",
    "print(X_test.shape)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(X_test.shape)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "4444e870e198520bbbd400b780cafb88571feb8b"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(X_train, Y_train, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "cd935e70f6d2185b65e478210381819df938cca7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(19, 256)\n",
    "        self.fc2 = nn.Linear(256, 156)\n",
    "        self.fc3 = nn.Linear(156, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.dropout(x, p=0.3)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = F.dropout(x, p=0.2)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = F.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "9d5641b90eccab8e9643e288c767984a539d63e4"
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "num_epochs = 81\n",
    "learning_rate = 0.01\n",
    "batch_no = len(x_train) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "938adf17cd24b2754282ca060cfb64a9df87c102"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "9f2d8485cc5aac6b03a1aca4c163b5c6f5cd4259"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from torch.autograd import Variable\n",
    "\n",
    "min_loss = np.Inf\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    x_train, y_train = shuffle(x_train, y_train)\n",
    "    \n",
    "    train_loss = 0\n",
    "    valid_loss = 0\n",
    "    \n",
    "    for i in range(batch_no):\n",
    "        x_var = Variable(torch.FloatTensor(x_train))\n",
    "        y_var = Variable(torch.LongTensor(y_train))\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        ypred_var = net(x_var)\n",
    "        loss =criterion(ypred_var, y_var)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        test_var = Variable(torch.FloatTensor(x_val), requires_grad=True)\n",
    "        test_y_var = Variable(torch.LongTensor(y_val))\n",
    "        with torch.no_grad():\n",
    "            result = net(test_var)\n",
    "            #loss\n",
    "            loss = criterion(result, test_y_var)\n",
    "            #calculate loss\n",
    "            valid_loss += loss.item()\n",
    "            \n",
    "            values, labels = torch.max(result, 1)\n",
    "            num_right = np.sum(labels.data.numpy() == y_val)\n",
    "            print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "                epoch+1, train_loss,valid_loss),\n",
    "                'Accuracy {:.2f}%'.format((num_right / len(y_val))* 100),)\n",
    "            \n",
    "        # save model if validation loss has decreased\n",
    "        if valid_loss <= min_loss:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "                min_loss,\n",
    "                valid_loss))\n",
    "            torch.save(net.state_dict(), 'model.pt')\n",
    "            min_loss = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load('model.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "b30b4c578b3ddd40ec9f63e19396b045c5ac0dd5"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "test_var = Variable(torch.FloatTensor(x_val), requires_grad=True)\n",
    "with torch.no_grad():\n",
    "    result = net(test_var)\n",
    "    values, labels = torch.max(result, 1)\n",
    "    num_right = np.sum(labels.data.numpy() == y_val)\n",
    "    print('Accuracy {:.2f}'.format(num_right / len(y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Survied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "10054399a4160803528137e2ebf60b73394b6007"
   },
   "outputs": [],
   "source": [
    "X_test_var = Variable(torch.FloatTensor(X_test), requires_grad=True)\n",
    "print(X_test_var.shape)\n",
    "with torch.no_grad():\n",
    "    test_result = net(X_test_var)\n",
    "values, labels = torch.max(test_result, 1)\n",
    "survived = labels.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "len(testset_passengers)\n",
    "len(survived)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "901d7d529bfe936d8c5091e2a73ffc9d1c5da8ed"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "data = [['PassengerId', 'Survived']]\n",
    "for i in range(len(survived)):\n",
    "    data.append([testset_passengers[i], survived[i]])\n",
    "    \n",
    "with open('submission.csv', 'w') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
