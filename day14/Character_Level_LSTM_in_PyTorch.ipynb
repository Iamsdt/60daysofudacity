{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "OvlAd5-tZsMK",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224.0
    },
    "outputId": "20860d30-b38b-4916-f519-d4ddab5db382"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-07-11 14:23:45--  https://raw.githubusercontent.com/udacity/deep-learning-v2-pytorch/master/recurrent-neural-networks/char-rnn/data/anna.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2025486 (1.9M) [text/plain]\n",
      "Saving to: ‘anna.txt’\n",
      "\n",
      "\ranna.txt              0%[                    ]       0  --.-KB/s               \ranna.txt            100%[===================>]   1.93M  --.-KB/s    in 0.04s   \n",
      "\n",
      "2019-07-11 14:23:46 (47.6 MB/s) - ‘anna.txt’ saved [2025486/2025486]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/udacity/deep-learning-v2-pytorch/master/recurrent-neural-networks/char-rnn/data/anna.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "z5Mwhm-_bVtP",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# open text file and read in data as `text`\n",
    "with open('anna.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "M8wdJDpebZ4e",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    },
    "outputId": "03ed302a-0bc5-4089-8174-f5a1a716e458"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "aYE-Z3pHbcIO",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "# encode the text\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3CGzAcHxbfLe",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119.0
    },
    "outputId": "f0b88557-b76f-41b2-857a-9268e0dbf410"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([48, 21, 75,  7, 49, 70, 80, 32, 76, 18, 18, 18, 81, 75,  7,  7, 20,\n",
       "       32, 56, 75,  1, 35, 65, 35, 70, 77, 32, 75, 80, 70, 32, 75, 65, 65,\n",
       "       32, 75, 65, 35,  4, 70, 30, 32, 70, 44, 70, 80, 20, 32, 43, 22, 21,\n",
       "       75,  7,  7, 20, 32, 56, 75,  1, 35, 65, 20, 32, 35, 77, 32, 43, 22,\n",
       "       21, 75,  7,  7, 20, 32, 35, 22, 32, 35, 49, 77, 32, 62, 66, 22, 18,\n",
       "       66, 75, 20, 27, 18, 18, 58, 44, 70, 80, 20, 49, 21, 35, 22])"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "P7Xk7G7dbl9o",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ftZCJMUubtfA",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68.0
    },
    "outputId": "555019b8-dfa7-45cd-9a8a-b58e7d6ab882"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# check that the function works as expected\n",
    "test_seq = np.array([[3, 5, 1]])\n",
    "one_hot = one_hot_encode(test_seq, 8)\n",
    "\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "Our-KvMMbyPE",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x seq_length from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       seq_length: Number of encoded chars in a sequence\n",
    "    '''\n",
    "    \n",
    "    batch_size_total = batch_size * seq_length\n",
    "    # total number of batches we can make\n",
    "    n_batches = len(arr)//batch_size_total\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size_total]\n",
    "    # Reshape into batch_size rows\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    # iterate through the array, one sequence at a time\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        # The features\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "Xcu2HkoDbzQw",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 8, 50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "gxcwAJzueZXX",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340.0
    },
    "outputId": "0d75179f-7f62-4642-c971-4de28edc6e2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[48 21 75  7 49 70 80 32 76 18]\n",
      " [77 62 22 32 49 21 75 49 32 75]\n",
      " [70 22 79 32 62 80 32 75 32 56]\n",
      " [77 32 49 21 70 32 82 21 35 70]\n",
      " [32 77 75 66 32 21 70 80 32 49]\n",
      " [82 43 77 77 35 62 22 32 75 22]\n",
      " [32 46 22 22 75 32 21 75 79 32]\n",
      " [15  6 65 62 22 77  4 20 27 32]]\n",
      "\n",
      "y\n",
      " [[21 75  7 49 70 80 32 76 18 18]\n",
      " [62 22 32 49 21 75 49 32 75 49]\n",
      " [22 79 32 62 80 32 75 32 56 62]\n",
      " [32 49 21 70 32 82 21 35 70 56]\n",
      " [77 75 66 32 21 70 80 32 49 70]\n",
      " [43 77 77 35 62 22 32 75 22 79]\n",
      " [46 22 22 75 32 21 75 79 32 77]\n",
      " [ 6 65 62 22 77  4 20 27 32 36]]\n"
     ]
    }
   ],
   "source": [
    "# printing out the first 10 items in a sequence\n",
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "VKCXq6SbebWt",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    },
    "outputId": "f8cf16f7-675c-4911-e8b3-922562ed3a9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "f2u7lKRKeiGV",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        # creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        ## TODO: define the LSTM\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        ## TODO: define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        ## TODO: define the final, fully-connected output layer\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "                \n",
    "        ## TODO: Get the outputs and the new hidden state from the lstm\n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        ## TODO: pass through a dropout layer\n",
    "        out = self.dropout(r_output)\n",
    "        \n",
    "        # Stack up LSTM outputs using view\n",
    "        # you may need to use contiguous to reshape the output\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        \n",
    "        ## TODO: put x through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "hYnYhbqtepOO",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "        seq_length: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                net.train() # reset to train mode after iterationg through validation data\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "HFuD86HqetvV",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102.0
    },
    "outputId": "c86652f9-ea34-4dce-d4bd-883d6d9349d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define and print the net\n",
    "n_hidden=512\n",
    "n_layers=2\n",
    "\n",
    "net = CharRNN(chars, n_hidden, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "mppGb2HMevLO",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000.0
    },
    "outputId": "c5ec4e53-14b9-4c77-ae7d-27abe7c25556"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20... Step: 10... Loss: 3.2548... Val Loss: 3.2188\n",
      "Epoch: 1/20... Step: 20... Loss: 3.1394... Val Loss: 3.1341\n",
      "Epoch: 1/20... Step: 30... Loss: 3.1405... Val Loss: 3.1235\n",
      "Epoch: 1/20... Step: 40... Loss: 3.1106... Val Loss: 3.1185\n",
      "Epoch: 1/20... Step: 50... Loss: 3.1465... Val Loss: 3.1173\n",
      "Epoch: 1/20... Step: 60... Loss: 3.1206... Val Loss: 3.1148\n",
      "Epoch: 1/20... Step: 70... Loss: 3.1052... Val Loss: 3.1128\n",
      "Epoch: 1/20... Step: 80... Loss: 3.1226... Val Loss: 3.1075\n",
      "Epoch: 1/20... Step: 90... Loss: 3.1178... Val Loss: 3.0996\n",
      "Epoch: 1/20... Step: 100... Loss: 3.0947... Val Loss: 3.0838\n",
      "Epoch: 1/20... Step: 110... Loss: 3.0592... Val Loss: 3.0362\n",
      "Epoch: 1/20... Step: 120... Loss: 2.9677... Val Loss: 2.9555\n",
      "Epoch: 1/20... Step: 130... Loss: 2.8449... Val Loss: 2.8064\n",
      "Epoch: 2/20... Step: 140... Loss: 2.7484... Val Loss: 2.6867\n",
      "Epoch: 2/20... Step: 150... Loss: 2.6262... Val Loss: 2.5775\n",
      "Epoch: 2/20... Step: 160... Loss: 2.5539... Val Loss: 2.5173\n",
      "Epoch: 2/20... Step: 170... Loss: 2.4832... Val Loss: 2.4651\n",
      "Epoch: 2/20... Step: 180... Loss: 2.4550... Val Loss: 2.4301\n",
      "Epoch: 2/20... Step: 190... Loss: 2.4124... Val Loss: 2.4245\n",
      "Epoch: 2/20... Step: 200... Loss: 2.4082... Val Loss: 2.3808\n",
      "Epoch: 2/20... Step: 210... Loss: 2.3700... Val Loss: 2.3435\n",
      "Epoch: 2/20... Step: 220... Loss: 2.3277... Val Loss: 2.3105\n",
      "Epoch: 2/20... Step: 230... Loss: 2.3238... Val Loss: 2.2989\n",
      "Epoch: 2/20... Step: 240... Loss: 2.2982... Val Loss: 2.2619\n",
      "Epoch: 2/20... Step: 250... Loss: 2.2298... Val Loss: 2.2284\n",
      "Epoch: 2/20... Step: 260... Loss: 2.2024... Val Loss: 2.2018\n",
      "Epoch: 2/20... Step: 270... Loss: 2.1996... Val Loss: 2.1776\n",
      "Epoch: 3/20... Step: 280... Loss: 2.1945... Val Loss: 2.1526\n",
      "Epoch: 3/20... Step: 290... Loss: 2.1616... Val Loss: 2.1254\n",
      "Epoch: 3/20... Step: 300... Loss: 2.1283... Val Loss: 2.1036\n",
      "Epoch: 3/20... Step: 310... Loss: 2.1014... Val Loss: 2.0818\n",
      "Epoch: 3/20... Step: 320... Loss: 2.0699... Val Loss: 2.0624\n",
      "Epoch: 3/20... Step: 330... Loss: 2.0422... Val Loss: 2.0440\n",
      "Epoch: 3/20... Step: 340... Loss: 2.0634... Val Loss: 2.0189\n",
      "Epoch: 3/20... Step: 350... Loss: 2.0433... Val Loss: 2.0024\n",
      "Epoch: 3/20... Step: 360... Loss: 1.9748... Val Loss: 1.9846\n",
      "Epoch: 3/20... Step: 370... Loss: 2.0007... Val Loss: 1.9710\n",
      "Epoch: 3/20... Step: 380... Loss: 1.9701... Val Loss: 1.9526\n",
      "Epoch: 3/20... Step: 390... Loss: 1.9441... Val Loss: 1.9377\n",
      "Epoch: 3/20... Step: 400... Loss: 1.9222... Val Loss: 1.9213\n",
      "Epoch: 3/20... Step: 410... Loss: 1.9342... Val Loss: 1.9058\n",
      "Epoch: 4/20... Step: 420... Loss: 1.9233... Val Loss: 1.8888\n",
      "Epoch: 4/20... Step: 430... Loss: 1.9180... Val Loss: 1.8720\n",
      "Epoch: 4/20... Step: 440... Loss: 1.8863... Val Loss: 1.8676\n",
      "Epoch: 4/20... Step: 450... Loss: 1.8423... Val Loss: 1.8510\n",
      "Epoch: 4/20... Step: 460... Loss: 1.8220... Val Loss: 1.8404\n",
      "Epoch: 4/20... Step: 470... Loss: 1.8564... Val Loss: 1.8286\n",
      "Epoch: 4/20... Step: 480... Loss: 1.8360... Val Loss: 1.8193\n",
      "Epoch: 4/20... Step: 490... Loss: 1.8476... Val Loss: 1.8045\n",
      "Epoch: 4/20... Step: 500... Loss: 1.8408... Val Loss: 1.7950\n",
      "Epoch: 4/20... Step: 510... Loss: 1.8109... Val Loss: 1.7843\n",
      "Epoch: 4/20... Step: 520... Loss: 1.8213... Val Loss: 1.7753\n",
      "Epoch: 4/20... Step: 530... Loss: 1.7773... Val Loss: 1.7643\n",
      "Epoch: 4/20... Step: 540... Loss: 1.7443... Val Loss: 1.7566\n",
      "Epoch: 4/20... Step: 550... Loss: 1.7900... Val Loss: 1.7451\n",
      "Epoch: 5/20... Step: 560... Loss: 1.7587... Val Loss: 1.7360\n",
      "Epoch: 5/20... Step: 570... Loss: 1.7415... Val Loss: 1.7268\n",
      "Epoch: 5/20... Step: 580... Loss: 1.7254... Val Loss: 1.7151\n",
      "Epoch: 5/20... Step: 590... Loss: 1.7255... Val Loss: 1.7103\n",
      "Epoch: 5/20... Step: 600... Loss: 1.7194... Val Loss: 1.7018\n",
      "Epoch: 5/20... Step: 610... Loss: 1.7004... Val Loss: 1.6960\n",
      "Epoch: 5/20... Step: 620... Loss: 1.7022... Val Loss: 1.6854\n",
      "Epoch: 5/20... Step: 630... Loss: 1.7163... Val Loss: 1.6828\n",
      "Epoch: 5/20... Step: 640... Loss: 1.6835... Val Loss: 1.6757\n",
      "Epoch: 5/20... Step: 650... Loss: 1.6717... Val Loss: 1.6650\n",
      "Epoch: 5/20... Step: 660... Loss: 1.6515... Val Loss: 1.6575\n",
      "Epoch: 5/20... Step: 670... Loss: 1.6862... Val Loss: 1.6551\n",
      "Epoch: 5/20... Step: 680... Loss: 1.6782... Val Loss: 1.6518\n",
      "Epoch: 5/20... Step: 690... Loss: 1.6468... Val Loss: 1.6409\n",
      "Epoch: 6/20... Step: 700... Loss: 1.6560... Val Loss: 1.6342\n",
      "Epoch: 6/20... Step: 710... Loss: 1.6405... Val Loss: 1.6288\n",
      "Epoch: 6/20... Step: 720... Loss: 1.6327... Val Loss: 1.6205\n",
      "Epoch: 6/20... Step: 730... Loss: 1.6396... Val Loss: 1.6210\n",
      "Epoch: 6/20... Step: 740... Loss: 1.6037... Val Loss: 1.6126\n",
      "Epoch: 6/20... Step: 750... Loss: 1.5960... Val Loss: 1.6069\n",
      "Epoch: 6/20... Step: 760... Loss: 1.6392... Val Loss: 1.6067\n",
      "Epoch: 6/20... Step: 770... Loss: 1.6162... Val Loss: 1.5997\n",
      "Epoch: 6/20... Step: 780... Loss: 1.5957... Val Loss: 1.5925\n",
      "Epoch: 6/20... Step: 790... Loss: 1.5748... Val Loss: 1.5856\n",
      "Epoch: 6/20... Step: 800... Loss: 1.5988... Val Loss: 1.5840\n",
      "Epoch: 6/20... Step: 810... Loss: 1.5828... Val Loss: 1.5777\n",
      "Epoch: 6/20... Step: 820... Loss: 1.5518... Val Loss: 1.5696\n",
      "Epoch: 6/20... Step: 830... Loss: 1.5989... Val Loss: 1.5680\n",
      "Epoch: 7/20... Step: 840... Loss: 1.5508... Val Loss: 1.5675\n",
      "Epoch: 7/20... Step: 850... Loss: 1.5634... Val Loss: 1.5593\n",
      "Epoch: 7/20... Step: 860... Loss: 1.5476... Val Loss: 1.5546\n",
      "Epoch: 7/20... Step: 870... Loss: 1.5567... Val Loss: 1.5483\n",
      "Epoch: 7/20... Step: 880... Loss: 1.5649... Val Loss: 1.5475\n",
      "Epoch: 7/20... Step: 890... Loss: 1.5528... Val Loss: 1.5437\n",
      "Epoch: 7/20... Step: 900... Loss: 1.5401... Val Loss: 1.5451\n",
      "Epoch: 7/20... Step: 910... Loss: 1.5107... Val Loss: 1.5405\n",
      "Epoch: 7/20... Step: 920... Loss: 1.5391... Val Loss: 1.5353\n",
      "Epoch: 7/20... Step: 930... Loss: 1.5241... Val Loss: 1.5311\n",
      "Epoch: 7/20... Step: 940... Loss: 1.5238... Val Loss: 1.5304\n",
      "Epoch: 7/20... Step: 950... Loss: 1.5338... Val Loss: 1.5240\n",
      "Epoch: 7/20... Step: 960... Loss: 1.5363... Val Loss: 1.5171\n",
      "Epoch: 7/20... Step: 970... Loss: 1.5383... Val Loss: 1.5154\n",
      "Epoch: 8/20... Step: 980... Loss: 1.5130... Val Loss: 1.5125\n",
      "Epoch: 8/20... Step: 990... Loss: 1.5101... Val Loss: 1.5079\n",
      "Epoch: 8/20... Step: 1000... Loss: 1.5004... Val Loss: 1.5049\n",
      "Epoch: 8/20... Step: 1010... Loss: 1.5393... Val Loss: 1.5039\n",
      "Epoch: 8/20... Step: 1020... Loss: 1.5142... Val Loss: 1.4978\n",
      "Epoch: 8/20... Step: 1030... Loss: 1.5000... Val Loss: 1.4975\n",
      "Epoch: 8/20... Step: 1040... Loss: 1.5009... Val Loss: 1.4984\n",
      "Epoch: 8/20... Step: 1050... Loss: 1.4819... Val Loss: 1.4938\n",
      "Epoch: 8/20... Step: 1060... Loss: 1.4896... Val Loss: 1.4901\n",
      "Epoch: 8/20... Step: 1070... Loss: 1.4913... Val Loss: 1.4852\n",
      "Epoch: 8/20... Step: 1080... Loss: 1.4858... Val Loss: 1.4832\n",
      "Epoch: 8/20... Step: 1090... Loss: 1.4798... Val Loss: 1.4788\n",
      "Epoch: 8/20... Step: 1100... Loss: 1.4648... Val Loss: 1.4723\n",
      "Epoch: 8/20... Step: 1110... Loss: 1.4701... Val Loss: 1.4717\n",
      "Epoch: 9/20... Step: 1120... Loss: 1.4842... Val Loss: 1.4700\n",
      "Epoch: 9/20... Step: 1130... Loss: 1.4735... Val Loss: 1.4669\n",
      "Epoch: 9/20... Step: 1140... Loss: 1.4711... Val Loss: 1.4628\n",
      "Epoch: 9/20... Step: 1150... Loss: 1.4914... Val Loss: 1.4608\n",
      "Epoch: 9/20... Step: 1160... Loss: 1.4421... Val Loss: 1.4600\n",
      "Epoch: 9/20... Step: 1170... Loss: 1.4534... Val Loss: 1.4583\n",
      "Epoch: 9/20... Step: 1180... Loss: 1.4462... Val Loss: 1.4616\n",
      "Epoch: 9/20... Step: 1190... Loss: 1.4792... Val Loss: 1.4553\n",
      "Epoch: 9/20... Step: 1200... Loss: 1.4291... Val Loss: 1.4497\n",
      "Epoch: 9/20... Step: 1210... Loss: 1.4433... Val Loss: 1.4494\n",
      "Epoch: 9/20... Step: 1220... Loss: 1.4409... Val Loss: 1.4473\n",
      "Epoch: 9/20... Step: 1230... Loss: 1.4226... Val Loss: 1.4461\n",
      "Epoch: 9/20... Step: 1240... Loss: 1.4284... Val Loss: 1.4397\n",
      "Epoch: 9/20... Step: 1250... Loss: 1.4389... Val Loss: 1.4387\n",
      "Epoch: 10/20... Step: 1260... Loss: 1.4410... Val Loss: 1.4368\n",
      "Epoch: 10/20... Step: 1270... Loss: 1.4351... Val Loss: 1.4349\n",
      "Epoch: 10/20... Step: 1280... Loss: 1.4505... Val Loss: 1.4323\n",
      "Epoch: 10/20... Step: 1290... Loss: 1.4402... Val Loss: 1.4303\n",
      "Epoch: 10/20... Step: 1300... Loss: 1.4249... Val Loss: 1.4291\n",
      "Epoch: 10/20... Step: 1310... Loss: 1.4324... Val Loss: 1.4278\n",
      "Epoch: 10/20... Step: 1320... Loss: 1.4043... Val Loss: 1.4335\n",
      "Epoch: 10/20... Step: 1330... Loss: 1.3963... Val Loss: 1.4302\n",
      "Epoch: 10/20... Step: 1340... Loss: 1.3994... Val Loss: 1.4233\n",
      "Epoch: 10/20... Step: 1350... Loss: 1.3856... Val Loss: 1.4196\n",
      "Epoch: 10/20... Step: 1360... Loss: 1.3854... Val Loss: 1.4195\n",
      "Epoch: 10/20... Step: 1370... Loss: 1.3876... Val Loss: 1.4185\n",
      "Epoch: 10/20... Step: 1380... Loss: 1.4216... Val Loss: 1.4101\n",
      "Epoch: 10/20... Step: 1390... Loss: 1.4399... Val Loss: 1.4077\n",
      "Epoch: 11/20... Step: 1400... Loss: 1.4356... Val Loss: 1.4144\n",
      "Epoch: 11/20... Step: 1410... Loss: 1.4543... Val Loss: 1.4103\n",
      "Epoch: 11/20... Step: 1420... Loss: 1.4390... Val Loss: 1.4063\n",
      "Epoch: 11/20... Step: 1430... Loss: 1.4071... Val Loss: 1.4089\n",
      "Epoch: 11/20... Step: 1440... Loss: 1.4254... Val Loss: 1.4041\n",
      "Epoch: 11/20... Step: 1450... Loss: 1.3490... Val Loss: 1.4052\n",
      "Epoch: 11/20... Step: 1460... Loss: 1.3748... Val Loss: 1.4064\n",
      "Epoch: 11/20... Step: 1470... Loss: 1.3696... Val Loss: 1.4044\n",
      "Epoch: 11/20... Step: 1480... Loss: 1.3875... Val Loss: 1.3993\n",
      "Epoch: 11/20... Step: 1490... Loss: 1.3777... Val Loss: 1.3960\n",
      "Epoch: 11/20... Step: 1500... Loss: 1.3760... Val Loss: 1.3983\n",
      "Epoch: 11/20... Step: 1510... Loss: 1.3435... Val Loss: 1.3962\n",
      "Epoch: 11/20... Step: 1520... Loss: 1.3849... Val Loss: 1.3915\n",
      "Epoch: 12/20... Step: 1530... Loss: 1.4336... Val Loss: 1.3899\n",
      "Epoch: 12/20... Step: 1540... Loss: 1.3888... Val Loss: 1.3876\n",
      "Epoch: 12/20... Step: 1550... Loss: 1.3967... Val Loss: 1.3864\n",
      "Epoch: 12/20... Step: 1560... Loss: 1.4035... Val Loss: 1.3834\n",
      "Epoch: 12/20... Step: 1570... Loss: 1.3555... Val Loss: 1.3856\n",
      "Epoch: 12/20... Step: 1580... Loss: 1.3258... Val Loss: 1.3871\n",
      "Epoch: 12/20... Step: 1590... Loss: 1.3270... Val Loss: 1.3824\n",
      "Epoch: 12/20... Step: 1600... Loss: 1.3487... Val Loss: 1.3849\n",
      "Epoch: 12/20... Step: 1610... Loss: 1.3395... Val Loss: 1.3856\n",
      "Epoch: 12/20... Step: 1620... Loss: 1.3461... Val Loss: 1.3812\n",
      "Epoch: 12/20... Step: 1630... Loss: 1.3668... Val Loss: 1.3769\n",
      "Epoch: 12/20... Step: 1640... Loss: 1.3478... Val Loss: 1.3797\n",
      "Epoch: 12/20... Step: 1650... Loss: 1.3271... Val Loss: 1.3761\n",
      "Epoch: 12/20... Step: 1660... Loss: 1.3803... Val Loss: 1.3733\n",
      "Epoch: 13/20... Step: 1670... Loss: 1.3514... Val Loss: 1.3721\n",
      "Epoch: 13/20... Step: 1680... Loss: 1.3572... Val Loss: 1.3685\n",
      "Epoch: 13/20... Step: 1690... Loss: 1.3452... Val Loss: 1.3689\n",
      "Epoch: 13/20... Step: 1700... Loss: 1.3335... Val Loss: 1.3682\n",
      "Epoch: 13/20... Step: 1710... Loss: 1.3116... Val Loss: 1.3694\n",
      "Epoch: 13/20... Step: 1720... Loss: 1.3358... Val Loss: 1.3711\n",
      "Epoch: 13/20... Step: 1730... Loss: 1.3745... Val Loss: 1.3642\n",
      "Epoch: 13/20... Step: 1740... Loss: 1.3373... Val Loss: 1.3671\n",
      "Epoch: 13/20... Step: 1750... Loss: 1.3000... Val Loss: 1.3721\n",
      "Epoch: 13/20... Step: 1760... Loss: 1.3252... Val Loss: 1.3681\n",
      "Epoch: 13/20... Step: 1770... Loss: 1.3567... Val Loss: 1.3628\n",
      "Epoch: 13/20... Step: 1780... Loss: 1.3229... Val Loss: 1.3625\n",
      "Epoch: 13/20... Step: 1790... Loss: 1.3075... Val Loss: 1.3652\n",
      "Epoch: 13/20... Step: 1800... Loss: 1.3285... Val Loss: 1.3583\n",
      "Epoch: 14/20... Step: 1810... Loss: 1.3428... Val Loss: 1.3565\n",
      "Epoch: 14/20... Step: 1820... Loss: 1.3204... Val Loss: 1.3549\n",
      "Epoch: 14/20... Step: 1830... Loss: 1.3427... Val Loss: 1.3524\n",
      "Epoch: 14/20... Step: 1840... Loss: 1.2912... Val Loss: 1.3550\n",
      "Epoch: 14/20... Step: 1850... Loss: 1.2709... Val Loss: 1.3527\n",
      "Epoch: 14/20... Step: 1860... Loss: 1.3340... Val Loss: 1.3544\n",
      "Epoch: 14/20... Step: 1870... Loss: 1.3341... Val Loss: 1.3504\n",
      "Epoch: 14/20... Step: 1880... Loss: 1.3229... Val Loss: 1.3526\n",
      "Epoch: 14/20... Step: 1890... Loss: 1.3453... Val Loss: 1.3565\n",
      "Epoch: 14/20... Step: 1900... Loss: 1.3238... Val Loss: 1.3481\n",
      "Epoch: 14/20... Step: 1910... Loss: 1.3209... Val Loss: 1.3472\n",
      "Epoch: 14/20... Step: 1920... Loss: 1.3259... Val Loss: 1.3489\n",
      "Epoch: 14/20... Step: 1930... Loss: 1.2797... Val Loss: 1.3487\n",
      "Epoch: 14/20... Step: 1940... Loss: 1.3412... Val Loss: 1.3485\n",
      "Epoch: 15/20... Step: 1950... Loss: 1.3062... Val Loss: 1.3478\n",
      "Epoch: 15/20... Step: 1960... Loss: 1.3128... Val Loss: 1.3413\n",
      "Epoch: 15/20... Step: 1970... Loss: 1.3031... Val Loss: 1.3391\n",
      "Epoch: 15/20... Step: 1980... Loss: 1.2840... Val Loss: 1.3416\n",
      "Epoch: 15/20... Step: 1990... Loss: 1.2879... Val Loss: 1.3372\n",
      "Epoch: 15/20... Step: 2000... Loss: 1.2778... Val Loss: 1.3420\n",
      "Epoch: 15/20... Step: 2010... Loss: 1.3002... Val Loss: 1.3378\n",
      "Epoch: 15/20... Step: 2020... Loss: 1.3118... Val Loss: 1.3444\n",
      "Epoch: 15/20... Step: 2030... Loss: 1.2828... Val Loss: 1.3441\n",
      "Epoch: 15/20... Step: 2040... Loss: 1.2963... Val Loss: 1.3378\n",
      "Epoch: 15/20... Step: 2050... Loss: 1.2936... Val Loss: 1.3326\n",
      "Epoch: 15/20... Step: 2060... Loss: 1.3060... Val Loss: 1.3342\n",
      "Epoch: 15/20... Step: 2070... Loss: 1.3068... Val Loss: 1.3356\n",
      "Epoch: 15/20... Step: 2080... Loss: 1.2936... Val Loss: 1.3350\n",
      "Epoch: 16/20... Step: 2090... Loss: 1.3011... Val Loss: 1.3316\n",
      "Epoch: 16/20... Step: 2100... Loss: 1.2866... Val Loss: 1.3276\n",
      "Epoch: 16/20... Step: 2110... Loss: 1.2763... Val Loss: 1.3276\n",
      "Epoch: 16/20... Step: 2120... Loss: 1.3009... Val Loss: 1.3307\n",
      "Epoch: 16/20... Step: 2130... Loss: 1.2699... Val Loss: 1.3300\n",
      "Epoch: 16/20... Step: 2140... Loss: 1.2718... Val Loss: 1.3301\n",
      "Epoch: 16/20... Step: 2150... Loss: 1.3031... Val Loss: 1.3298\n",
      "Epoch: 16/20... Step: 2160... Loss: 1.2834... Val Loss: 1.3304\n",
      "Epoch: 16/20... Step: 2170... Loss: 1.2810... Val Loss: 1.3303\n",
      "Epoch: 16/20... Step: 2180... Loss: 1.2761... Val Loss: 1.3270\n",
      "Epoch: 16/20... Step: 2190... Loss: 1.2997... Val Loss: 1.3250\n",
      "Epoch: 16/20... Step: 2200... Loss: 1.2693... Val Loss: 1.3230\n",
      "Epoch: 16/20... Step: 2210... Loss: 1.2440... Val Loss: 1.3251\n",
      "Epoch: 16/20... Step: 2220... Loss: 1.2848... Val Loss: 1.3247\n",
      "Epoch: 17/20... Step: 2230... Loss: 1.2600... Val Loss: 1.3200\n",
      "Epoch: 17/20... Step: 2240... Loss: 1.2635... Val Loss: 1.3181\n",
      "Epoch: 17/20... Step: 2250... Loss: 1.2616... Val Loss: 1.3184\n",
      "Epoch: 17/20... Step: 2260... Loss: 1.2720... Val Loss: 1.3196\n",
      "Epoch: 17/20... Step: 2270... Loss: 1.2743... Val Loss: 1.3204\n",
      "Epoch: 17/20... Step: 2280... Loss: 1.2822... Val Loss: 1.3184\n",
      "Epoch: 17/20... Step: 2290... Loss: 1.2763... Val Loss: 1.3192\n",
      "Epoch: 17/20... Step: 2300... Loss: 1.2428... Val Loss: 1.3217\n",
      "Epoch: 17/20... Step: 2310... Loss: 1.2620... Val Loss: 1.3169\n",
      "Epoch: 17/20... Step: 2320... Loss: 1.2570... Val Loss: 1.3115\n",
      "Epoch: 17/20... Step: 2330... Loss: 1.2572... Val Loss: 1.3124\n",
      "Epoch: 17/20... Step: 2340... Loss: 1.2739... Val Loss: 1.3189\n",
      "Epoch: 17/20... Step: 2350... Loss: 1.2832... Val Loss: 1.3152\n",
      "Epoch: 17/20... Step: 2360... Loss: 1.2768... Val Loss: 1.3151\n",
      "Epoch: 18/20... Step: 2370... Loss: 1.2605... Val Loss: 1.3117\n",
      "Epoch: 18/20... Step: 2380... Loss: 1.2655... Val Loss: 1.3096\n",
      "Epoch: 18/20... Step: 2390... Loss: 1.2588... Val Loss: 1.3099\n",
      "Epoch: 18/20... Step: 2400... Loss: 1.2811... Val Loss: 1.3124\n",
      "Epoch: 18/20... Step: 2410... Loss: 1.2814... Val Loss: 1.3135\n",
      "Epoch: 18/20... Step: 2420... Loss: 1.2561... Val Loss: 1.3087\n",
      "Epoch: 18/20... Step: 2430... Loss: 1.2630... Val Loss: 1.3096\n",
      "Epoch: 18/20... Step: 2440... Loss: 1.2539... Val Loss: 1.3123\n",
      "Epoch: 18/20... Step: 2450... Loss: 1.2604... Val Loss: 1.3067\n",
      "Epoch: 18/20... Step: 2460... Loss: 1.2592... Val Loss: 1.3082\n",
      "Epoch: 18/20... Step: 2470... Loss: 1.2543... Val Loss: 1.3043\n",
      "Epoch: 18/20... Step: 2480... Loss: 1.2507... Val Loss: 1.3073\n",
      "Epoch: 18/20... Step: 2490... Loss: 1.2271... Val Loss: 1.3035\n",
      "Epoch: 18/20... Step: 2500... Loss: 1.2435... Val Loss: 1.3039\n",
      "Epoch: 19/20... Step: 2510... Loss: 1.2533... Val Loss: 1.3006\n",
      "Epoch: 19/20... Step: 2520... Loss: 1.2656... Val Loss: 1.3011\n",
      "Epoch: 19/20... Step: 2530... Loss: 1.2659... Val Loss: 1.3010\n",
      "Epoch: 19/20... Step: 2540... Loss: 1.2804... Val Loss: 1.3020\n",
      "Epoch: 19/20... Step: 2550... Loss: 1.2349... Val Loss: 1.3071\n",
      "Epoch: 19/20... Step: 2560... Loss: 1.2468... Val Loss: 1.3011\n",
      "Epoch: 19/20... Step: 2570... Loss: 1.2356... Val Loss: 1.3037\n",
      "Epoch: 19/20... Step: 2580... Loss: 1.2731... Val Loss: 1.3047\n",
      "Epoch: 19/20... Step: 2590... Loss: 1.2293... Val Loss: 1.3014\n",
      "Epoch: 19/20... Step: 2600... Loss: 1.2334... Val Loss: 1.3004\n",
      "Epoch: 19/20... Step: 2610... Loss: 1.2496... Val Loss: 1.2986\n",
      "Epoch: 19/20... Step: 2620... Loss: 1.2274... Val Loss: 1.3006\n",
      "Epoch: 19/20... Step: 2630... Loss: 1.2268... Val Loss: 1.2986\n",
      "Epoch: 19/20... Step: 2640... Loss: 1.2542... Val Loss: 1.2970\n",
      "Epoch: 20/20... Step: 2650... Loss: 1.2475... Val Loss: 1.2949\n",
      "Epoch: 20/20... Step: 2660... Loss: 1.2486... Val Loss: 1.2968\n",
      "Epoch: 20/20... Step: 2670... Loss: 1.2584... Val Loss: 1.2916\n",
      "Epoch: 20/20... Step: 2680... Loss: 1.2469... Val Loss: 1.2983\n",
      "Epoch: 20/20... Step: 2690... Loss: 1.2353... Val Loss: 1.2969\n",
      "Epoch: 20/20... Step: 2700... Loss: 1.2543... Val Loss: 1.2924\n",
      "Epoch: 20/20... Step: 2710... Loss: 1.2234... Val Loss: 1.2913\n",
      "Epoch: 20/20... Step: 2720... Loss: 1.2203... Val Loss: 1.2979\n",
      "Epoch: 20/20... Step: 2730... Loss: 1.2110... Val Loss: 1.2951\n",
      "Epoch: 20/20... Step: 2740... Loss: 1.2128... Val Loss: 1.2985\n",
      "Epoch: 20/20... Step: 2750... Loss: 1.2205... Val Loss: 1.2953\n",
      "Epoch: 20/20... Step: 2760... Loss: 1.2134... Val Loss: 1.2966\n",
      "Epoch: 20/20... Step: 2770... Loss: 1.2514... Val Loss: 1.2933\n",
      "Epoch: 20/20... Step: 2780... Loss: 1.2775... Val Loss: 1.2927\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "seq_length = 100\n",
    "n_epochs = 20 # start smaller if you are just testing initial behavior\n",
    "\n",
    "# train the model\n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "ZiObh8ghhf5E",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# change the name, for saving multiple files\n",
    "model_name = 'rnn_20_epoch.net'\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "G05lZLhjhfvE",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "        \n",
    "        # get top characters\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "Ffo0tDrRhs-3",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None):\n",
    "        \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "gbmV8D8xhve1",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377.0
    },
    "outputId": "6475329c-9a0b-42eb-b3c8-84130efbae18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna, the crush there was the ministry\n",
      "and heard, and was not at once another wife, this thought that was not a could not come in the man when he seemed, talking to all\n",
      "her, and he would come to a minute.\n",
      "\n",
      "He trinked the concentrating the sorry, the carriage was\n",
      "stoping.\n",
      "\n",
      "\"And I don't beait my sere consider. We am has her from the meadow,\" then the part had been saying his bridegriem.\n",
      "\n",
      "\"In his franking table in\n",
      "the same sign, a man would\n",
      "be a servant of it,\" said Lizaveta Pecrovna, he supposed talking of the subject of his head.\n",
      "\n",
      "At the same and with his strange silence in the sides as a matter\n",
      "and all of the frond of a long walking of the change and sole on the simple that had a more of all at that his carriage. At the moment were the fact that the fourdie changes the courtes were\n",
      "answer for that home too he seemed so here, took the point, but something had taken any\n",
      "merries, and he was settling it with his\n",
      "beautiful\n",
      "figure. And, so such a musural came of them. He went up to\n",
      "the children w\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 1000, prime='Anna', top_k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "BG46AmE_h7h-",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    },
    "outputId": "72ee33ed-56b4-420d-d814-0f4ce75b3dfb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('rnn_20_epoch.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "    \n",
    "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "loaded.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "viIVEmNoiAUt",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 751.0
    },
    "outputId": "046ecd6d-ebc0-4c6a-c01a-f329edfebf28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And Shudipto said: \"I don't know that I'm going\n",
      "for the money\n",
      "than in\n",
      "that\n",
      "sister-are three steps of\n",
      "mysilf in all her tastes and such a conformance, and all about that man a child having to be so fancied the state of home. He was truthing the might humble, where they work and the children, and when the sale that he was not to\n",
      "suppose the man strong at those answing to the\n",
      "strength. All the possible stronger took\n",
      "her to her frunt, and the\n",
      "princess were never\n",
      "from the sick men with herself too, was it, and was not any most influence to be all hos and horses and somewhat he felt\n",
      "and anywhere, and all together this mistake the steps of the moment hust before, he went into her face. She wanted to speak to Levin, and how he saw it. The formed house as a shill was horrible to the\n",
      "memory.\n",
      "\n",
      "Stepan Arkadyevitch, which she sat several times, he would not come on\n",
      "them with the same steps.\n",
      "\n",
      "\"You would have train to me to be so in the pispose\n",
      "to me the sister of him. What is seeing her?\"\n",
      "\n",
      "And he was not talking about it.\n",
      "\n",
      "\"When I do not say to her, and I'm sorry,\" said Levin, as he saw something what the sourder taking her hand shat fros her from her.\n",
      "\n",
      "\"We made to speak of it.\"\n",
      "\n",
      "And the more would not have taken offershels, and went to him, he called the from the children, with which he was an imprysing\n",
      "one of the creature was a calming\n",
      "service\n",
      "was not almess the first post to the same and at a position of the\n",
      "cords of the straight that would have to be at a cheak, and the countess, as a\n",
      "minute and a moment. But that was not to be, holding the streed and straight all the same\n",
      "time of\n",
      "a peasant and horse and the society. He\n",
      "was not for his feeling and the country and all he had been said, the country. To say simply but as she felt a chair, his\n",
      "bard conversation was superficed.\n",
      "\n",
      "\"I'm a sudgen.\"\n",
      "\n",
      "\"Ah! why do?\"\n",
      "\n",
      "\"Oh, then the mideles was a lot at talking about any party, I don't know you, but to say that I'm the significance of them. In the son, and he's an instant to might to met you, I should be th\n"
     ]
    }
   ],
   "source": [
    "# Sample using a loaded model\n",
    "print(sample(loaded, 2000, top_k=5, prime=\"And Shudipto said\"))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Character-Level LSTM in PyTorch.ipynb",
   "version": "0.3.2",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
