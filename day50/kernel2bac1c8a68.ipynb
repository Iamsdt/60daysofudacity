{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\nimport numpy as np\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"os.listdir('../input/himu-book')","execution_count":2,"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"['himu.txt']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('../input/himu-book/himu.txt', 'r') as f:\n    text = f.read()","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text[:500]","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"'\\nপ্রসঙ্গ হিমু\\n\\n\\n\\n\\tহিমু আমার প্রিয় চরিত্রের একটি। যখন হিমুকে নিয়ে কিছু লিখি-- নিজেকে হিমু মনে হয়, একধরনের ঘোর অনুভব করি। এই ব্যাপারটা অন্য কোন লেখার সময় তেমন ঘটে না। হিমুকে নিয়ে আমার প্রথম লেখা ময়ুরাক্ষি। ময়ুরাক্ষি লেখার সময় ব্যাপারটা প্রথম লক্ষ করি। দ্বিতীয়বারে লিখলাম রজার ওপাশে। তখনো একই ব্যাপার। কেন এরকম হয়? মানুষ হিসেবে আমি যুক্তিবাদী। হিমু যুক্তিহীন, রহস্যময় জগৎ একজন যুক্তিবাদীকে কেন আকর্ষণ করবে? আমার জানা নেই। যদি কখনও জানতে পারি-- পাঠকদের জানাব।\\n\\n\\n\\n\\tহুমায়ূন আহমেদ\\n\\n\\tএলিফেন্ট রোড\\n\\n\\n\\n\\n\\nকি নাম'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"chars = tuple(set(text))\nint2char = dict(enumerate(chars))\nchar2int = {ch: ii for ii, ch in int2char.items()}\n\n# encode the text\nencoded = np.array([char2int[ch] for ch in text])\n\nencoded[:100]","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"array([  0,  41,  95,  36,  53,  29,  95,  81,  25,   1,  59,  99, 100,\n         0,   0,   0,   0, 108,   1,  59,  99, 100,  25,   9,  99,  42,\n        36,  25,  41,  95,  36,  59,  27,  25,  18,  36,  59,  16,  95,\n        36,   2,  36,  25,  50,  93,  14,  59,  55,  25,  40,  24,   7,\n        25,   1,  59,  99, 100,  93,   2,  25,   7,  59,  27,   2,  25,\n        93,  59, 101, 100,  25,  63,  59,  24,  59,  83,  83,  25,   7,\n        59,  17,   2,  93,   2,  25,   1,  59,  99, 100,  25,  99,   7,\n         2,  25,   1,  27,  54,  25,  50,  93,   6])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def one_hot_encode(arr, n_labels):\n    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n    \n    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n    \n    one_hot = one_hot.reshape((*arr.shape, n_labels))\n    \n    return one_hot","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_seq = np.array([[3, 5, 1]])\none_hot = one_hot_encode(test_seq, 8)\n\nprint(one_hot)","execution_count":7,"outputs":[{"output_type":"stream","text":"[[[0. 0. 0. 1. 0. 0. 0. 0.]\n  [0. 0. 0. 0. 0. 1. 0. 0.]\n  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_batches(arr, batch_size, seq_length):\n    \n    batch_size_total = batch_size * seq_length\n    \n    n_batches = len(arr)//batch_size_total\n    arr = arr[:n_batches * batch_size_total]\n    arr = arr.reshape((batch_size, -1))\n    \n    for n in range(0, arr.shape[1], seq_length):\n        x = arr[:, n:n+seq_length]\n        y = np.zeros_like(x)\n        try:\n            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n        except IndexError:\n            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n        yield x, y","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batches = get_batches(encoded, 10, 50)\nx, y = next(batches)","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('x\\n', x[:10, :10])\nprint('\\ny\\n', y[:10, :10])","execution_count":10,"outputs":[{"output_type":"stream","text":"x\n [[  0  41  95  36  53  29  95  81  25   1]\n [ 42   7   2  25  90  42  16  25  24   2]\n [ 54  25  86  42  75  36 100  99   2  25]\n [ 27   2  25  50  53   2 101  59  63  42]\n [ 25   9 101   2  55  25   9  36  25  24]\n [100  40   2  42  81  25   9  41   7  42]\n [ 99  55  25  50  99  42  25   4  36  17]\n [  2  25   1  86   2  25  41  42 107  18]\n [ 25  93  36   2 101   2   7  55  25  16]\n [ 44   7  42  36  25  21  26 100   6  41]]\n\ny\n [[ 41  95  36  53  29  95  81  25   1  59]\n [  7   2  25  90  42  16  25  24   2  16]\n [ 25  86  42  75  36 100  99   2  25  70]\n [  2  25  50  53   2 101  59  63  42  99]\n [  9 101   2  55  25   9  36  25  24  42]\n [ 40   2  42  81  25   9  41   7  42  93]\n [ 55  25  50  99  42  25   4  36  17  42]\n [ 25   1  86   2  25  41  42 107  18  55]\n [ 93  36   2 101   2   7  55  25  16  59]\n [  7  42  36  25  21  26 100   6  41  16]]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_on_gpu = torch.cuda.is_available()","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CharRNN(nn.Module):\n    \n    def __init__(self, tokens, n_hidden=256, n_layers=2,\n                               drop_prob=0.4, lr=0.001):\n        super().__init__()\n        self.drop_prob = drop_prob\n        self.n_layers = n_layers\n        self.n_hidden = n_hidden\n        self.lr = lr\n        \n        self.chars = tokens\n        self.int2char = dict(enumerate(self.chars))\n        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n        \n        self.lstm1 = nn.LSTM(len(self.chars), n_hidden, n_layers, \n                            dropout=drop_prob, batch_first=True)\n        \n        self.dropout = nn.Dropout(drop_prob)\n        \n        self.fc1 = nn.Linear(n_hidden, int(n_hidden/2))\n        self.fc2 = nn.Linear(int(n_hidden/2), int(n_hidden/2))\n        self.fc3 = nn.Linear(int(n_hidden/2), len(self.chars))\n      \n    \n    def forward(self, x, hidden):\n        \n        r_output, hidden = self.lstm1(x, hidden)\n        \n        out = self.dropout(r_output)\n        \n        out = out.contiguous().view(-1, self.n_hidden)\n        \n        out = self.fc1(out)\n        out = self.fc2(out)\n        out = self.fc3(out)\n        \n        return out, hidden\n    \n    \n    def init_hidden(self, batch_size):\n        weight = next(self.parameters()).data\n        \n        if (train_on_gpu):\n            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n        else:\n            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n        \n        return hidden","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define and print the net\nn_hidden=1024\nn_layers=5\n\nnet = CharRNN(chars, n_hidden, n_layers)\nprint(net)","execution_count":28,"outputs":[{"output_type":"stream","text":"CharRNN(\n  (lstm1): LSTM(109, 1024, num_layers=5, batch_first=True, dropout=0.4)\n  (dropout): Dropout(p=0.4, inplace=False)\n  (fc1): Linear(in_features=1024, out_features=512, bias=True)\n  (fc2): Linear(in_features=512, out_features=512, bias=True)\n  (fc3): Linear(in_features=512, out_features=109, bias=True)\n)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(net, data, epochs=10, batch_size=10, seq_length=10, lr=0.001, clip=5, val_frac=0.15, print_every=10):\n    \n    net.train()\n    \n    opt = torch.optim.Adam(net.parameters(), lr=lr)\n    criterion = nn.CrossEntropyLoss()\n    \n    # create training and validation data\n    val_idx = int(len(data)*(1-val_frac))\n    data, val_data = data[:val_idx], data[val_idx:]\n    \n    if(train_on_gpu):\n        net.cuda()\n    \n    counter = 0\n    n_chars = len(net.chars)\n    for e in range(epochs):\n        # initialize hidden state\n        h = net.init_hidden(batch_size)\n        \n        for x, y in get_batches(data, batch_size, seq_length):\n            counter += 1\n            \n            # One-hot encode our data and make them Torch tensors\n            x = one_hot_encode(x, n_chars)\n            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n            \n            if(train_on_gpu):\n                inputs, targets = inputs.cuda(), targets.cuda()\n\n            # Creating new variables for the hidden state, otherwise\n            # we'd backprop through the entire training history\n            h = tuple([each.data for each in h])\n\n            # zero accumulated gradients\n            net.zero_grad()\n            \n            # get the output from the model\n            output, h = net(inputs, h)\n            \n            # calculate the loss and perform backprop\n            loss = criterion(output, targets.view(batch_size*seq_length).long())\n            loss.backward()\n            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n            nn.utils.clip_grad_norm_(net.parameters(), clip)\n            opt.step()\n            \n            # loss stats\n            if counter % print_every == 0:\n                # Get validation loss\n                val_h = net.init_hidden(batch_size)\n                val_losses = []\n                net.eval()\n                for x, y in get_batches(val_data, batch_size, seq_length):\n                    # One-hot encode our data and make them Torch tensors\n                    x = one_hot_encode(x, n_chars)\n                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n                    \n                    # Creating new variables for the hidden state, otherwise\n                    # we'd backprop through the entire training history\n                    val_h = tuple([each.data for each in val_h])\n                    \n                    inputs, targets = x, y\n                    if(train_on_gpu):\n                        inputs, targets = inputs.cuda(), targets.cuda()\n\n                    output, val_h = net(inputs, val_h)\n                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n                \n                    val_losses.append(val_loss.item())\n                \n                net.train() # reset to train mode after iterationg through validation data\n                \n                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n                      \"Step: {}...\".format(counter),\n                      \"Loss: {:.4f}...\".format(loss.item()),\n                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 128\nseq_length = 10\nn_epochs = 70\n\n# train the model\ntrain(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=50)","execution_count":30,"outputs":[{"output_type":"stream","text":"Epoch: 1/70... Step: 50... Loss: 3.4128... Val Loss: 3.4643\nEpoch: 2/70... Step: 100... Loss: 3.0587... Val Loss: 3.1076\nEpoch: 2/70... Step: 150... Loss: 2.7304... Val Loss: 2.7201\nEpoch: 3/70... Step: 200... Loss: 2.5849... Val Loss: 2.5793\nEpoch: 4/70... Step: 250... Loss: 2.4357... Val Loss: 2.4940\nEpoch: 4/70... Step: 300... Loss: 2.3664... Val Loss: 2.3872\nEpoch: 5/70... Step: 350... Loss: 2.2948... Val Loss: 2.3186\nEpoch: 6/70... Step: 400... Loss: 2.2119... Val Loss: 2.2166\nEpoch: 6/70... Step: 450... Loss: 2.0461... Val Loss: 2.1320\nEpoch: 7/70... Step: 500... Loss: 2.0327... Val Loss: 2.0771\nEpoch: 8/70... Step: 550... Loss: 1.9828... Val Loss: 2.0311\nEpoch: 8/70... Step: 600... Loss: 1.8668... Val Loss: 1.9707\nEpoch: 9/70... Step: 650... Loss: 1.8193... Val Loss: 1.9362\nEpoch: 10/70... Step: 700... Loss: 1.6953... Val Loss: 1.9234\nEpoch: 10/70... Step: 750... Loss: 1.7355... Val Loss: 1.8849\nEpoch: 11/70... Step: 800... Loss: 1.6648... Val Loss: 1.8923\nEpoch: 12/70... Step: 850... Loss: 1.5431... Val Loss: 1.8756\nEpoch: 12/70... Step: 900... Loss: 1.5533... Val Loss: 1.8562\nEpoch: 13/70... Step: 950... Loss: 1.5705... Val Loss: 1.8389\nEpoch: 14/70... Step: 1000... Loss: 1.6437... Val Loss: 1.8156\nEpoch: 14/70... Step: 1050... Loss: 1.4929... Val Loss: 1.8461\nEpoch: 15/70... Step: 1100... Loss: 1.4707... Val Loss: 1.8611\nEpoch: 16/70... Step: 1150... Loss: 1.4629... Val Loss: 1.8506\nEpoch: 16/70... Step: 1200... Loss: 1.2820... Val Loss: 1.9011\nEpoch: 17/70... Step: 1250... Loss: 1.3263... Val Loss: 1.8948\nEpoch: 18/70... Step: 1300... Loss: 1.3493... Val Loss: 1.9028\nEpoch: 18/70... Step: 1350... Loss: 1.2933... Val Loss: 1.9176\nEpoch: 19/70... Step: 1400... Loss: 1.3314... Val Loss: 1.9490\nEpoch: 20/70... Step: 1450... Loss: 1.2846... Val Loss: 1.9517\nEpoch: 20/70... Step: 1500... Loss: 1.1672... Val Loss: 1.9543\nEpoch: 21/70... Step: 1550... Loss: 1.1266... Val Loss: 1.9618\nEpoch: 22/70... Step: 1600... Loss: 1.1674... Val Loss: 1.9831\nEpoch: 22/70... Step: 1650... Loss: 1.1388... Val Loss: 1.9887\nEpoch: 23/70... Step: 1700... Loss: 1.0712... Val Loss: 2.0572\nEpoch: 24/70... Step: 1750... Loss: 1.0969... Val Loss: 2.0908\nEpoch: 24/70... Step: 1800... Loss: 1.0267... Val Loss: 2.0947\nEpoch: 25/70... Step: 1850... Loss: 1.0167... Val Loss: 2.1167\nEpoch: 25/70... Step: 1900... Loss: 1.2054... Val Loss: 2.1697\nEpoch: 26/70... Step: 1950... Loss: 1.0306... Val Loss: 2.1927\nEpoch: 27/70... Step: 2000... Loss: 0.9582... Val Loss: 2.1752\nEpoch: 27/70... Step: 2050... Loss: 0.9215... Val Loss: 2.2674\nEpoch: 28/70... Step: 2100... Loss: 0.8138... Val Loss: 2.3121\nEpoch: 29/70... Step: 2150... Loss: 0.8430... Val Loss: 2.2529\nEpoch: 29/70... Step: 2200... Loss: 0.8331... Val Loss: 2.3033\nEpoch: 30/70... Step: 2250... Loss: 0.7734... Val Loss: 2.4005\nEpoch: 31/70... Step: 2300... Loss: 0.8804... Val Loss: 2.3124\nEpoch: 31/70... Step: 2350... Loss: 0.7300... Val Loss: 2.3735\nEpoch: 32/70... Step: 2400... Loss: 0.7377... Val Loss: 2.4926\nEpoch: 33/70... Step: 2450... Loss: 0.7570... Val Loss: 2.4555\nEpoch: 33/70... Step: 2500... Loss: 0.7321... Val Loss: 2.4896\nEpoch: 34/70... Step: 2550... Loss: 0.6671... Val Loss: 2.5725\nEpoch: 35/70... Step: 2600... Loss: 0.6707... Val Loss: 2.5141\nEpoch: 35/70... Step: 2650... Loss: 0.6947... Val Loss: 2.5791\nEpoch: 36/70... Step: 2700... Loss: 0.7006... Val Loss: 2.5923\nEpoch: 37/70... Step: 2750... Loss: 0.6141... Val Loss: 2.5889\nEpoch: 37/70... Step: 2800... Loss: 0.6601... Val Loss: 2.6751\nEpoch: 38/70... Step: 2850... Loss: 0.5816... Val Loss: 2.6259\nEpoch: 39/70... Step: 2900... Loss: 0.6711... Val Loss: 2.5992\nEpoch: 39/70... Step: 2950... Loss: 0.6048... Val Loss: 2.7623\nEpoch: 40/70... Step: 3000... Loss: 0.5328... Val Loss: 2.6724\nEpoch: 41/70... Step: 3050... Loss: 0.5523... Val Loss: 2.6972\nEpoch: 41/70... Step: 3100... Loss: 0.5506... Val Loss: 2.8231\nEpoch: 42/70... Step: 3150... Loss: 0.5189... Val Loss: 2.7747\nEpoch: 43/70... Step: 3200... Loss: 0.5550... Val Loss: 2.7915\nEpoch: 43/70... Step: 3250... Loss: 0.5453... Val Loss: 2.9084\nEpoch: 44/70... Step: 3300... Loss: 0.5555... Val Loss: 2.8493\nEpoch: 45/70... Step: 3350... Loss: 0.5079... Val Loss: 2.8744\nEpoch: 45/70... Step: 3400... Loss: 0.4910... Val Loss: 2.9487\nEpoch: 46/70... Step: 3450... Loss: 0.4553... Val Loss: 2.8874\nEpoch: 47/70... Step: 3500... Loss: 0.4514... Val Loss: 2.9603\nEpoch: 47/70... Step: 3550... Loss: 0.4366... Val Loss: 2.9931\nEpoch: 48/70... Step: 3600... Loss: 0.4690... Val Loss: 2.9259\nEpoch: 49/70... Step: 3650... Loss: 0.4824... Val Loss: 3.0023\nEpoch: 49/70... Step: 3700... Loss: 0.4664... Val Loss: 3.0419\nEpoch: 50/70... Step: 3750... Loss: 0.4005... Val Loss: 2.9626\nEpoch: 50/70... Step: 3800... Loss: 0.5400... Val Loss: 3.0646\nEpoch: 51/70... Step: 3850... Loss: 0.4090... Val Loss: 3.0485\nEpoch: 52/70... Step: 3900... Loss: 0.4386... Val Loss: 3.0180\nEpoch: 52/70... Step: 3950... Loss: 0.4297... Val Loss: 3.1258\nEpoch: 53/70... Step: 4000... Loss: 0.4088... Val Loss: 3.0960\nEpoch: 54/70... Step: 4050... Loss: 0.3685... Val Loss: 3.1145\nEpoch: 54/70... Step: 4100... Loss: 0.3904... Val Loss: 3.1749\nEpoch: 55/70... Step: 4150... Loss: 0.3775... Val Loss: 3.1606\nEpoch: 56/70... Step: 4200... Loss: 0.3642... Val Loss: 3.1368\nEpoch: 56/70... Step: 4250... Loss: 0.3995... Val Loss: 3.1883\nEpoch: 57/70... Step: 4300... Loss: 0.3864... Val Loss: 3.2005\nEpoch: 58/70... Step: 4350... Loss: 0.3800... Val Loss: 3.1672\nEpoch: 58/70... Step: 4400... Loss: 0.3434... Val Loss: 3.2840\nEpoch: 59/70... Step: 4450... Loss: 0.3701... Val Loss: 3.2341\nEpoch: 60/70... Step: 4500... Loss: 0.3581... Val Loss: 3.2357\nEpoch: 60/70... Step: 4550... Loss: 0.3543... Val Loss: 3.2733\nEpoch: 61/70... Step: 4600... Loss: 0.3603... Val Loss: 3.2191\nEpoch: 62/70... Step: 4650... Loss: 0.3223... Val Loss: 3.2209\nEpoch: 62/70... Step: 4700... Loss: 0.2811... Val Loss: 3.3149\nEpoch: 63/70... Step: 4750... Loss: 0.2941... Val Loss: 3.2798\nEpoch: 64/70... Step: 4800... Loss: 0.3437... Val Loss: 3.3111\nEpoch: 64/70... Step: 4850... Loss: 0.3255... Val Loss: 3.3595\nEpoch: 65/70... Step: 4900... Loss: 0.3182... Val Loss: 3.2684\nEpoch: 66/70... Step: 4950... Loss: 0.3670... Val Loss: 3.3520\nEpoch: 66/70... Step: 5000... Loss: 0.3284... Val Loss: 3.3551\nEpoch: 67/70... Step: 5050... Loss: 0.3517... Val Loss: 3.2773\nEpoch: 68/70... Step: 5100... Loss: 0.3159... Val Loss: 3.3768\nEpoch: 68/70... Step: 5150... Loss: 0.2910... Val Loss: 3.3811\nEpoch: 69/70... Step: 5200... Loss: 0.2819... Val Loss: 3.3236\nEpoch: 70/70... Step: 5250... Loss: 0.3295... Val Loss: 3.3814\nEpoch: 70/70... Step: 5300... Loss: 0.2975... Val Loss: 3.3691\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(net, char, h=None, top_k=None):\n        ''' Given a character, predict the next character.\n            Returns the predicted character and the hidden state.\n        '''\n        \n        # tensor inputs\n        x = np.array([[net.char2int[char]]])\n        x = one_hot_encode(x, len(net.chars))\n        inputs = torch.from_numpy(x)\n        \n        if(train_on_gpu):\n            inputs = inputs.cuda()\n        \n        # detach hidden state from history\n        h = tuple([each.data for each in h])\n        # get the output of the model\n        out, h = net(inputs, h)\n\n        # get the character probabilities\n        p = F.softmax(out, dim=1).data\n        if(train_on_gpu):\n            p = p.cpu() # move to cpu\n        \n        # get top characters\n        if top_k is None:\n            top_ch = np.arange(len(net.chars))\n        else:\n            p, top_ch = p.topk(top_k)\n            top_ch = top_ch.numpy().squeeze()\n        \n        # select the likely next character with some element of randomness\n        p = p.numpy().squeeze()\n        char = np.random.choice(top_ch, p=p/p.sum())\n        \n        # return the encoded value of the predicted char and the hidden state\n        return net.int2char[char], h","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sample(net, size, prime='হিমু ', top_k=None):\n        \n    if(train_on_gpu):\n        net.cuda()\n    else:\n        net.cpu()\n    \n    net.eval() # eval mode\n    \n    # First off, run through the prime characters\n    chars = [ch for ch in prime]\n    h = net.init_hidden(1)\n    for ch in prime:\n        char, h = predict(net, ch, h, top_k=top_k)\n\n    chars.append(char)\n    \n    # Now pass in the previous character and get a new one\n    for ii in range(size):\n        char, h = predict(net, chars[-1], h, top_k=top_k)\n        chars.append(char)\n\n    return ''.join(chars)","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sample(net, 1000, prime='হিমু ', top_k=5))","execution_count":34,"outputs":[{"output_type":"stream","text":"হিমু শুরু করুন।’\n\n\tআমি বললাম, আপনার অফিসে গাষ করেছি। আজ হল আমার ঘুম-দিবস।’\n\n\tজীবনবাবু নড়লেন না। চুপচাপ দাঁড়িয়ে রইলেন। আমি বললাম, আরো কিছু বলবেন?\n\n\t‘জ্বি-না।’\n\n\t‘আপনার কী ধারণা ডাক্তার সাহেব? সে বলল, মানুষটার জন্যে হয়তো বয়স বেশি লাগছে? কেন জানি মাথায় ভোঁতা ধরণের যন্ত্রনা হচ্ছে। চিঠি পাঠিয়েছে ইংরেজিতে টেই মেয়ে। এখন সেটা শুরু হয়েছে। আমি মুহিব সাহেব মশারির ডাণ্ডা খুলি না।’\n\n\t‘আমার কাছে কলম আছে।’\n\n\t‘আপনার কী ধারণা ডাক্তার সাহেব?’\n\n\t‘আপনার সঙ্গে খানিকক্ষন কথাবার্তা না। একটু বসুন। আপনি কী চান নাইয়া দেরি করেন। আমি বললাম, আপনি কেমন আছেন?’\n\n\tএমা যারামন্য পানি ফেলেছে। তাকে হাসপাতালে নিয়ে মাথা ব্যান্ডেজ করে বাসায়ে পৌঁছে েন্টের আসছে হয় মাথাধরার যেলিফোন শুরু করবেন।’\n\n\t‘আপনার দাদীমা ভাল আছেন?’\n\n\t‘হ্যাঁ, ভালই আছেন।’\n\n\t‘কবে আসবেন?’\n\n\t‘খাওয়াদাওয়া করছে?’\n\n\t‘প্রথম দিন কিছু খাননি। রাতে একটা মুখ লাল হত। খামার সামনে আমার জন্যে মেসে সকালবেলা চা হয় না। চা খেতে রাস্তার ওপাশে ক্যান্টিনে যেতে ইচ্ছা করে কাউকে কিছু বলছেন? তাস ওদেরকে কি না-খেলতে বলব?’\n\n\t‘না।’\n\n\t‘আমি অবশ্যি মা’কে খুন হতে দেখিনি। তেমন কোনো প্রমাণও পাইনি। বি\n","name":"stdout"}]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}